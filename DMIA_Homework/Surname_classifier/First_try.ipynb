{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0\n"
     ]
    }
   ],
   "source": [
    "'''!=x[1:2].isupper()'''\n",
    "def istitle(x):\n",
    "    if x[0:1].isupper():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "print(istitle('Привет'),istitle('ПРивет'),istitle('пРивет'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "surnames=pd.read_csv(r\"/home/yury/Repository/Sandbox/DMIA_Homework/Surname_classifier/linear_train.txt\", names=['Word','Lastname'],sep=',')\n",
    "test=pd.read_csv(r\"/home/yury/Repository/Sandbox/DMIA_Homework/Surname_classifier/linear_test.txt\", names=['Word'],sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(surnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188920\n",
      "101408\n",
      "101408\n",
      "x: ['Аалто' 'ААР' 'Аара' 'Ааре' 'Аарон' 'Аароне' 'Ааронов' 'Аароном'\n",
      " 'Аароном' 'Аарону'] ... ['ёлку' 'ёлочек' 'ёлочка' 'ёлочки' 'ёмкостей' 'ёмкости' 'Ёмкости'\n",
      " 'Ёмкость' 'ёмкостью' 'ёмкостях']\n",
      "x: ['Аалтонен' 'Аар' 'Аарон' 'ААРОН' 'Аарона' 'Аарона' 'Аароне' 'Ааронов'\n",
      " 'Аахена' 'Абабков'] ... ['ящичках' 'ящичков' 'ящичку' 'ящур' 'ящура' 'Ёлкин' 'ёлкой' 'ёлок'\n",
      " 'ёлочкой' 'ёмкость']\n",
      "y: [1 0 0 0 0 1 0 0 0 1] ... [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "x_test=test.Word.values\n",
    "x_train=surnames.Word.values\n",
    "y_train=surnames.Lastname.values\n",
    "print(len(x_test))\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print('x:',x_test[:10],'...',x_test[-10:])\n",
    "print('x:',x_train[:10],'...',x_train[-10:])\n",
    "print('y:',y_train[:10],'...',y_train[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_titled(x):\n",
    "    firstlet=[]\n",
    "    for i in x:\n",
    "        firstlet.append([istitle(i)])\n",
    "    return firstlet\n",
    "is_titled(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_shape_by_syl(x):\n",
    "    vectorizer1=CountVectorizer(analyzer='word',min_df=50,max_df=10000)\n",
    "    last_syls1 = vectorizer1.fit_transform(words_by_syls(x,-1))\n",
    "    last_syls1 = last_syls1.toarray()\n",
    "    vectorizer2=CountVectorizer(analyzer='word',min_df=100,max_df=10000)\n",
    "    last_syls1 = vectorizer1.fit_transform(words_by_syls(x,-1))\n",
    "    last_syls2 = last_syls2.toarray()\n",
    "    vectorizer3=CountVectorizer(analyzer='word',min_df=500,max_df=10000)\n",
    "    last_syls1 = vectorizer1.fit_transform(words_by_syls(x,-1))\n",
    "    last_syls3 = last_syls3.toarray()\n",
    "    return np.hstack((is_titled(x),last_syls1,last_syls2,last_syls3)),vectorizer1,vectorizer2,vectorizer3,None,None\n",
    "def get_in_shape_by_char(x):\n",
    "    vectorizer1=CountVectorizer(analyzer='char',min_df=2)\n",
    "    last_syls1 = vectorizer1.fit_transform(words_by_chars(x,-1))\n",
    "    last_syls1 = last_syls1.toarray()\n",
    "    vectorizer2=CountVectorizer(analyzer='char',min_df=2)\n",
    "    last_syls2 = vectorizer2.fit_transform(words_by_chars(x,-2,-1))\n",
    "    last_syls2 = last_syls2.toarray()\n",
    "    vectorizer3=CountVectorizer(analyzer='char',min_df=2)\n",
    "    last_syls3 = vectorizer3.fit_transform(words_by_chars(x,-3,-2))\n",
    "    last_syls3 = last_syls3.toarray()\n",
    "    vectorizer4=CountVectorizer(analyzer='char',min_df=2)\n",
    "    last_syls4 = vectorizer4.fit_transform(words_by_chars(x,-4,-3))\n",
    "    last_syls4 = last_syls4.toarray()\n",
    "    vectorizer5=CountVectorizer(analyzer='char',min_df=2)\n",
    "    last_syls5 = vectorizer5.fit_transform(words_by_chars(x,-5,-4))\n",
    "    last_syls5 = last_syls5.toarray()\n",
    "    return np.hstack((is_titled(x),last_syls1, last_syls2,last_syls3)),vectorizer1,vectorizer2,vectorizer3,vectorizer4,vectorizer5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_shape_by_syl_fin(x):\n",
    "    last_syls1 = vectorizer1.transform(words_by_syls(x,-1))\n",
    "    last_syls1 = last_syls1.toarray()\n",
    "    last_syls1 = vectorizer1.transform(words_by_syls(x,-1))\n",
    "    last_syls2 = last_syls2.toarray()\n",
    "    last_syls1 = vectorizer1.transform(words_by_syls(x,-1))\n",
    "    last_syls3 = last_syls3.toarray()\n",
    "    return np.hstack((is_titled(x),last_syls1,last_syls2,last_syls3))\n",
    "def get_in_shape_by_char_fin(x):\n",
    "    last_syls1 = vectorizer1.transform(words_by_chars(x,-1))\n",
    "    last_syls1 = last_syls1.toarray()\n",
    "    last_syls2 = vectorizer2.transform(words_by_chars(x,-2,-1))\n",
    "    last_syls2 = last_syls2.toarray()\n",
    "    last_syls3 = vectorizer3.transform(words_by_chars(x,-3,-2))\n",
    "    last_syls3 = last_syls3.toarray()\n",
    "    last_syls4 = vectorizer4.transform(words_by_chars(x,-4,-3))\n",
    "    last_syls4 = last_syls4.toarray()\n",
    "    last_syls5 = vectorizer5.transform(words_by_chars(x,-5,-4))\n",
    "    last_syls5 = last_syls5.toarray()\n",
    "    return np.hstack((is_titled(x),last_syls1, last_syls2,last_syls3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'last_syls2' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c1225261342b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_shaped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_in_shape_by_syl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dense matrix shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-53d0760659f1>\u001b[0m in \u001b[0;36mget_in_shape_by_syl\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvectorizer2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlast_syls1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_by_syls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlast_syls2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_syls2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvectorizer3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlast_syls1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_by_syls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'last_syls2' referenced before assignment"
     ]
    }
   ],
   "source": [
    "x_shaped,vectorizer1,vectorizer2,vectorizer3,vectorizer4,vectorizer5=get_in_shape_by_syl(x_train)\n",
    "print('Dense matrix shape', x_shaped.data.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df=pd.DataFrame(last_syls)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.to_csv('test.csv',sep=',')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "test=pd.read_csv(r\"/home/yury/Repository/Sandbox/DMIA_Homework/Surname_classifier/test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train=test.values[:,1:-1]\n",
    "y_train=test.values[:,-1]\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print('x:',x_train[:10],'...',x_train[-10:])\n",
    "print('y:',y_train[:10],'...',y_train[-10:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_2_words = {\n",
    "    v: k\n",
    "    for k, v in vectorizer.vocabulary_.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('num_2_words:',num_2_words[1],'...',num_2_words[10],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator # чтобы поддержать интерфейс sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor # для обученияb на каждой итерации\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "def der_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "def der_log_loss(y_hat, y_true):\n",
    "    return y_true*(1/y_hat) - (1-y_true)/(1-y_hat)\n",
    "def calc_gradient(y_hat, y_true):\n",
    "    return der_log_loss(sigmoid(y_hat), y_true) * der_sigmoid(y_hat)\n",
    "class SimpleGB(BaseEstimator):\n",
    "    def __init__(self, tree_params_dict, iters=100, tau=1e-1):\n",
    "        \"\"\"\n",
    "        tree_params_dict - словарь параметров, которые надо использовать при обучении дерева на итерации\n",
    "        iters - количество итераций\n",
    "        tau - коэффициент перед предсказаниями деревьев на каждой итерации\n",
    "        \"\"\"\n",
    "        self.tree_params_dict = tree_params_dict\n",
    "        self.iters = iters\n",
    "        self.tau = tau\n",
    "        \n",
    "    def fit(self, X_data, y_data):\n",
    "        self.estimators = []\n",
    "        curr_pred = 0\n",
    "        for iter_num in range(self.iters):\n",
    "            # Нужно найти градиент функции потерь по предсказниям в точке curr_pred\n",
    "            grad = calc_gradient(curr_pred, y_data)\n",
    "            # Мы максимизируем, поэтому надо обучить DecisionTreeRegressor с параметрами \n",
    "            # tree_params_dict по X_data предсказывать grad\n",
    "            algo = DecisionTreeRegressor(**self.tree_params_dict).fit(X_data,grad)\n",
    "            self.estimators.append(algo)\n",
    "            # все предсказания домножаются на tau и обновляется переменная curr_pred\n",
    "            curr_pred += self.tau * algo.predict(X_data)\n",
    "    \n",
    "    def predict(self, X_data):\n",
    "        # изначально все предскзания нули\n",
    "        res = np.zeros(X_data.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            # нужно сложить все предсказания деревьев с весом self.tau\n",
    "            res+=estimator.predict(X_data)*self.tau\n",
    "            \n",
    "        return (res > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algor = SimpleGB(\n",
    "    tree_params_dict={'max_depth':4},\n",
    "    iters=10,\n",
    "    tau = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "algor = LogisticRegression(penalty='l1', C=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "arr = cross_val_score(algor, x_shaped, y_train, cv=5, scoring='roc_auc')\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algor.fit(x_shaped,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=algor.predict(get_in_shape_by_syl__fin(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algor.predict(get_in_shape_by_syl_fin(['глушко']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_test[:2000]:\n",
    "    print(i,algor.predict(get_in_shape_by_char_fin([i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_test,columns=['Answer'],dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Submissions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
